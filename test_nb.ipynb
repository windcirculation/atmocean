{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import requests\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "from job_bot import postbot\n",
    "from api_token import api_key\n",
    "import pandas as pd\n",
    "\n",
    "# def init_bot():\n",
    "bot_token = api_key()\n",
    "\n",
    "def egujobs(post_jobs=True, jobbot_status=False, current_date=None):\n",
    "    if jobbot_status:\n",
    "        sign1 = f\"\\n----- ** {datetime.now().strftime('%Y-%b-%d')} ** -----\"\n",
    "        sign2 = f\"\\nEGU Jobs\"\n",
    "        message_text = f\" *{'Job_Bot Status: Active'}* {sign1}{sign2}\"\n",
    "        postbot(bot_token, message_text)\n",
    "\n",
    "    if post_jobs:        \n",
    "        if current_date is None:\n",
    "            # Get the current time in the Hawaii timezone\n",
    "            hawaii_timezone = pytz.timezone('Pacific/Honolulu')\n",
    "            current_date_hawaii = datetime.now(hawaii_timezone)    \n",
    "            current_date = current_date_hawaii.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        base_url0 = 'https://www.egu.eu/jobs/?'\n",
    "        base_url1 = 'limit=10&sortby=-created_at&sortby=-created_at&'\n",
    "        base_url2 = 'page=&keywords=&sector=10&employment_level=30'\n",
    "        base_url = f'{base_url0}{base_url1}{base_url2}'\n",
    "\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(base_url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the elements you want to scrape\n",
    "            job_list = soup.find_all('div', class_='media-body')\n",
    "\n",
    "            for job in job_list:\n",
    "                job_title = job.find('h2').text.strip()\n",
    "                job_link = urljoin(base_url, job.find('h2').find('a')['href'])\n",
    "\n",
    "                # Send a request to the job page\n",
    "                job_response = requests.get(job_link)\n",
    "                if job_response.status_code == 200:\n",
    "                    job_soup = BeautifulSoup(job_response.text, 'html.parser')\n",
    "                    # Location\n",
    "\n",
    "                    # Find the \"Posted\" line\n",
    "                    posted_line = job_soup.find(\"div\",\n",
    "                                                class_=\"col-md-3 mb-2 mb-md-0 strong text-md-right\",\n",
    "                                                string='Posted')\n",
    "\n",
    "                    # Find the date that follows the \"Posted\" line\n",
    "                    if posted_line:\n",
    "                        posted_date = posted_line.find_next_sibling('div').text.strip()\n",
    "                        input_format = \"%d %B %Y\"  # day month year (e.g., \"29 September 2023\")\n",
    "                        # Parse the posted_date into a datetime object\n",
    "                        posted_datetime = datetime.strptime(posted_date,\n",
    "                                                            input_format).strftime('%Y-%m-%d')\n",
    "                        posted_datetime =  datetime.strptime(posted_datetime, \"%Y-%m-%d\")\n",
    "                        converted_date = datetime.strptime(current_date, \"%Y-%m-%d\")\n",
    "                        if posted_datetime == converted_date:\n",
    "                            # print(posted_datetime)\n",
    "                            location = job_soup.find(\"div\",\n",
    "                                class_=\"col-md-3 mb-2 mb-md-0 strong text-md-right\",\n",
    "                                string=\"Location\"\n",
    "                                )\n",
    "\n",
    "                            location_content = location.find_next_sibling(\n",
    "                                \"div\").text.strip() if location else \"Not specified\"\n",
    "\n",
    "                            # Application deadline\n",
    "                            application_deadline = job_soup.find(\n",
    "                                \"div\",\n",
    "                                class_=\"col-md-3 mb-2 mb-md-0 strong text-md-right\",\n",
    "                                string=\"Application deadline\")\n",
    "\n",
    "                            application_deadline_content = application_deadline.find_next_sibling(\n",
    "                                \"div\").text.strip() if application_deadline else \"Not specified\"\n",
    "\n",
    "                            # Job description\n",
    "                            job_description = job_soup.find(\n",
    "                                \"div\",\n",
    "                                class_=\"col-md-3 mb-2 mb-md-0 strong text-md-right\",\n",
    "                                string=\"Job description\")\n",
    "\n",
    "                            job_description_content = job_description.find_next_sibling(\n",
    "                                \"div\").text.strip() if job_description else \"Not specified\"\n",
    "\n",
    "                            # How to apply\n",
    "                            how_to_apply = job_soup.find(\n",
    "                                \"div\",\n",
    "                                class_=\"col-md-3 mb-2 mb-md-0 strong text-md-right\",\n",
    "                                string=\"How to apply\"\n",
    "                            )\n",
    "\n",
    "                            how_to_apply_content = how_to_apply.find_next_sibling(\n",
    "                                \"div\").text.strip() if how_to_apply else \"Not specified\"\n",
    "\n",
    "                            # # Print the extracted information\n",
    "                            # print(f\"Title: {job_title}\")\n",
    "                            # print(f\"Date Posted: {posted_date}\")\n",
    "                            # print(f\"Location: {location_content}\")\n",
    "                            # print(f\"Deadline: {application_deadline_content}\")\n",
    "                            # print(f\"Job Description:\\n{job_description_content}\")\n",
    "                            # print(f\"How to Apply:\\n{how_to_apply_content}\")\n",
    "\n",
    "                            job_heading = f\"*{job_title}* \\n\\nLocation:\\\n",
    "                            {location_content} \\nDate Posted: {posted_date} \\\n",
    "                            \\nDeadline: {application_deadline_content}\"\n",
    "                            job_desc = f\"\\n\\nDescription:\\n{job_description_content} \\\n",
    "                            \\n\\nHow to Apply:\\n{how_to_apply_content}\\n--------\"\n",
    "\n",
    "                            full_post = job_heading+job_desc+\"---------\"\n",
    "\n",
    "                            postbot(bot_token, full_post)\n",
    "                            # print(full_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Post-doctoral researcher in ocean ensemble modeling* \n",
      "\n",
      "Location:                            Grenoble, France \n",
      "Date Posted: 14 November 2023                             \n",
      "Deadline: Open until the position is filled\n",
      "\n",
      "Description:\n",
      "Missions:\n",
      "This post-doctorate concerns the study of the impact of different sources of uncertainty on the physical modeling of the Mediterranean by producing and analysing ensembles of regional NEMO simulations at 1/12°. Once these ensembles have been compared with available spatial and in situ observations using probabilistic scores, they will be used to quantify the relative and cumulative impacts of these uncertainties on the simulated physics and on the amplitude and structure of ensemble dispersions.\n",
      "The activities of the selected person will concern:\n",
      "- getting to grips with the stochastic ensemble simulator of the Mediterranean, which is currently being developed on the basis of an existing realistic configuration of the NEMO 1/12° model;\n",
      "- participating in the calibration and implementation in this configuration of stochastic sources of uncertainty, linked to initial conditions, atmospheric forcing and unresolved scales, for example;\n",
      "- producing ensemble simulations including different uncertainties, and evaluating them with respect to observations;\n",
      "- quantifying and characterising the oceanic impacts of these uncertainties by physical and statistical analyses of the ensembles and their inter-comparison;\n",
      "- presenting these results in scientific articles and at international conferences.\n",
      "This work will be based on the expertise of the host team’s researchers and engineers on NEMO, and on the methods and tools (ensemblist, stochastic, statistical, uncertainty management, comparison with observations, etc.) that have been developed over the last fifteen years as part of research projects such as OCCIPUT, IMMERSE, and SEAMLESS.\n",
      "Context:\n",
      "This post-doctorate is part of the MEDIATION project funded by the PPR Océan et Climat. MEDIATION is developing methods and tools allowing to perform multi-decadal projections of the physics, biogeochemistry and biology of the Channel-Biscay and Mediterranean ocean basins, explicitly taking into account various uncertainties and evolution scenarios (climate change, human activities). The numerical tools developed as part of the project are also intended to contribute to improved interaction between science, society and politics.\n",
      "The successful candidate will join the MEOM team of IGE, whose research focuses on numerical oceanography, ocean/sea-ice modelling and forecasting. She or he will be working closely with Thierry Penduff, Pierre Brasseur (CNRS researchers), and Jean-Michel Brankart (CNRS research engineer), and will be interacting regularly with the other MEDIATION participants.                             \n",
      "\n",
      "How to Apply:\n",
      "To apply, please use this link (ignore the Dec 5 2023 deadline: the offer will stay valid until fulfilled).\n",
      "Skills:\n",
      "- PhD in physical oceanography\n",
      "- Experience in ocean modelling on a supercomputer\n",
      "- Aptitude for teamwork\n",
      "- Interest in the statistical analysis of ensemble simulations\n",
      "- Interest in disseminating scientific results\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "egujobs(current_date=\"2023-11-14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pytz\n",
    "from job_bot import postbot\n",
    "from api_token import api_key\n",
    "\n",
    "# def init_bot():\n",
    "bot_token = api_key()\n",
    "\n",
    "def esjobs(post_jobs=True, jobbot_status=False, current_date=None):\n",
    "    if jobbot_status:\n",
    "        sign1 = f\"\\n----- ** {datetime.now().strftime('%Y-%b-%d')} ** -----\"\n",
    "        sign2 = f\"\\nEarth Science Jobs\"\n",
    "        message_text = f\" *{'Job_Bot Status: Active'}* {sign1}{sign2}\"\n",
    "        postbot(bot_token, message_text)\n",
    "    \n",
    "    if post_jobs:\n",
    "        if current_date is None:\n",
    "            # Set the time zone to Hawaii Standard Time (HST)\n",
    "            hawaii_timezone = pytz.timezone('Pacific/Honolulu')\n",
    "\n",
    "            # Get the current time in the Hawaii timezone\n",
    "            current_date = datetime.now(hawaii_timezone)    \n",
    "            print(current_date)\n",
    "            converted_date = datetime.strptime(current_date, \"%Y-%m-%d\")\n",
    "            \n",
    "        current_time = converted_date.strftime(\"%B %Y\")\n",
    "        # print(current_time)\n",
    "\n",
    "        url = \"https://mailman.ucar.edu/pipermail/es_jobs_net/\"\n",
    "\n",
    "        # Define user-agent and headers\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; \\\n",
    "            x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,\\\n",
    "            application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9,ar;q=0.8\",\n",
    "            \"Cache-Control\": \"max-age=0\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Host\": \"mailman.ucar.edu\",\n",
    "            \"If-Modified-Since\": \"Thu, 19 Oct 2023 09:12:44 GMT\",\n",
    "            \"If-None-Match\": \"\\\"1a8c-6080e289aa9c3\\\"\",\n",
    "            \"Sec-Ch-Ua\": \"\\\"Chromium\\\";v=\\\"118\\\", \\\"Google Chrome\\\";v=\\\"118\\\", \\\"Not=A?Brand\\\";v=\\\"99\\\"\",\n",
    "            \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "            \"Sec-Ch-Ua-Platform\": \"\\\"macOS\\\"\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\"\n",
    "        }\n",
    "\n",
    "        # Send a request to the URL\n",
    "        response = requests.get(url, headers=headers)\n",
    "        page_content = response.text\n",
    "\n",
    "        soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "\n",
    "        # Find the link to the latest thread (October 2023)\n",
    "        latest_thread_link = None\n",
    "        for row in soup.find_all(\"tr\"):\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells and current_time in cells[0].get_text():\n",
    "                latest_thread_link = row.find(\"a\", href=True)[\"href\"]\n",
    "                break\n",
    "\n",
    "        tempPath = latest_thread_link.split(\"/\")[0]\n",
    "        print(tempPath)\n",
    "\n",
    "        # Construct the URL for the latest thread\n",
    "        if latest_thread_link:\n",
    "            latest_thread_url = f\"{url.rstrip('/')}/{latest_thread_link}\"\n",
    "\n",
    "\n",
    "            # Send a request to the latest thread's URL and scrape its content\n",
    "            response_latest_thread = requests.get(latest_thread_url, headers=headers)\n",
    "            latest_thread_content = response_latest_thread.text\n",
    "\n",
    "            # Parse and extract data from the latest thread's content using BeautifulSoup\n",
    "            latest_thread_soup = BeautifulSoup(latest_thread_content, \"html.parser\")\n",
    "\n",
    "            # Define keywords to search for\n",
    "            keywords = [\"phd\", \"ph.d.\", \"doctoral\", \"ms \", \"m.s.\", \"master\", \"assistantship\", \"fellowship\"]\n",
    "\n",
    "            # Initialize a list to store the links containing the keywords\n",
    "            filtered_links = []\n",
    "\n",
    "            # Loop through the links and check if they contain any of the keywords\n",
    "            for link in latest_thread_soup.find_all(\"a\", href=True):  # Fixed this line\n",
    "                for keyword in keywords:\n",
    "                    if re.search(keyword, link.get_text(), re.I):\n",
    "                        filtered_links.append(link['href'])\n",
    "\n",
    "            # Print the filtered links\n",
    "            for link in filtered_links:\n",
    "        #             print(f\"posting {link}\")\n",
    "                # print(os.path.join(url, tempPath, link))\n",
    "                full_url = os.path.join(url, tempPath, link)\n",
    "                # Send a request to the filtered link and scrape its content\n",
    "                response_filtered_link = requests.get(full_url, headers=headers)\n",
    "                filtered_link_content = response_filtered_link.text\n",
    "\n",
    "                # Parse and extract data from the filtered link's content using BeautifulSoup\n",
    "                filtered_link_soup = BeautifulSoup(filtered_link_content, \"html.parser\")\n",
    "                # You can perform additional parsing or extraction here, or simply print the content\n",
    "                # print(\"Content of filtered link:\", full_url)        \n",
    "        #         print(filtered_link_soup.get_text())  # Print the text content of the filtered link\n",
    "    #             print(\"--------------------------------------\\n\")\n",
    "\n",
    "                pub_date = filtered_link_soup.find(\"i\").get_text()\n",
    "                format_str = '%a %b %d %H:%M:%S %Y'\n",
    "                date_time_str = pub_date[:19]+pub_date[23:]\n",
    "    #             print(len(date_time_str))\n",
    "                if len(date_time_str) == 24:\n",
    "                    date_time_ = datetime.strptime(date_time_str, format_str).strftime(\"%Y-%m-%d\")\n",
    "                    date_time_ = datetime.strptime(date_time_, \"%Y-%m-%d\")\n",
    "                    converted_date = datetime.strptime(current_date, \"%Y-%m-%d\")\n",
    "                    if date_time_ == converted_date:\n",
    "        #                 print(f\"found for {date_time_}\")\n",
    "\n",
    "\n",
    "                        # Extract the header\n",
    "                        header = filtered_link_soup.find(\"h1\")  # Assuming the header is within an h2 element\n",
    "                        if header:\n",
    "                            header_text = header.get_text().removeprefix('[ES_JOBS_NET] ')\n",
    "                            # print(\"Header:\")\n",
    "                            # print(header_text)\n",
    "\n",
    "                        # Extract the body\n",
    "                        body = filtered_link_soup.find(\"pre\")  # Adjust class as needed\n",
    "                        if body:\n",
    "                            body_text = body.get_text()\n",
    "                            # print(\"\\nBody:\")\n",
    "                            split_index = body_text.find(\"-------------- next part --------------\")\n",
    "                            if split_index != -1:\n",
    "                            # Keep only the part before \"-------------- next part --------------\"\n",
    "                                body_text = body_text[:split_index]\n",
    "                            # print(body_text)\n",
    "                        sign1 = f\"\\n----- ** {datetime.now().strftime('%Y-%b-%d')} ** -----\"\n",
    "                        sign2 = f\"\\n\"\n",
    "                        message_text = f\" *{header_text.title()}* \\n\\n{body_text}{sign1}{sign2}\"\n",
    "                        # postbot(bot_token, message_text)\n",
    "                        print(message_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-16 10:08:47.872001-10:00\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "strptime() argument 1 must be str, not datetime.datetime",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m esjobs(current_date\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     current_date \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow(hawaii_timezone)    \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(current_date)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     converted_date \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39;49mstrptime(current_date, \u001b[39m\"\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY-\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mm-\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m current_time \u001b[39m=\u001b[39m converted_date\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mB \u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# print(current_time)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: strptime() argument 1 must be str, not datetime.datetime"
     ]
    }
   ],
   "source": [
    "esjobs(current_date=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from job_bot import postbot\n",
    "from api_token import api_key\n",
    "import feedparser\n",
    "import pytz\n",
    "\n",
    "\n",
    "bot_token = api_key()\n",
    "\n",
    "def metjobs(post_jobs=True, jobbot_status=False, current_date=None):\n",
    "    if jobbot_status:\n",
    "        sign1 = f\"\\n----- ** {datetime.now().strftime('%Y-%b-%d')} ** -----\"\n",
    "        sign2 = f\"\\nMet-Jobs\"\n",
    "        message_text = f\" *{'Job_Bot Status: Active'}* {sign1}{sign2}\"\n",
    "        postbot(bot_token, message_text)\n",
    "    \n",
    "    if post_jobs:\n",
    "        if current_date is None:\n",
    "            # Set the time zone to Hawaii Standard Time (HST)\n",
    "            hawaii_timezone = pytz.timezone('Pacific/Honolulu')\n",
    "            current_date = datetime.now(hawaii_timezone).strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            current_date = datetime.strptime(current_date, \"%Y-%m-%d\")\n",
    "        # Replace this URL with the URL of the RSS feed you want to scrape\n",
    "        rss_url = \"https://maillists.reading.ac.uk/scripts/wa-READING.exe?RSS&L=MET-JOBS&v=2.0&LIMIT=50\"\n",
    "\n",
    "        # Define headers for your requests to mimic a web browser\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'en-US,en;q=0.9,ar;q=0.8',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'Referer': 'http://localhost:8888/',\n",
    "            'Sec-Ch-Ua': '\"Chromium\";v=\"118\", \"Google Chrome\";v=\"118\", \"Not=A?Brand\";v=\"99\"',\n",
    "            'Sec-Ch-Ua-Mobile': '?0',\n",
    "            'Sec-Ch-Ua-Platform': '\"macOS\"',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'cross-site',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'authority': 'maillists.reading.ac.uk',\n",
    "            'method': 'GET',\n",
    "            'path': '/scripts/wa-READING.exe?A2=MET-JOBS;45592065.2310D',\n",
    "            'scheme': 'https',\n",
    "            'Cookie': 'WALOGIN=RESET; WALOGIN=RESET',\n",
    "        }\n",
    "\n",
    "        # Parse the RSS feed\n",
    "        feed = feedparser.parse(rss_url)\n",
    "\n",
    "        # Extract and print the information for each item\n",
    "        for item in feed.entries:\n",
    "            pub_date = item.published\n",
    "            author = item.author\n",
    "            title = item.title\n",
    "            guid = item.id\n",
    "            description = item.description\n",
    "            # print(pub_date)\n",
    "            # print(pub_date[:24])\n",
    "            # Convert the pub_date string to a datetime object\n",
    "            # pub_date_datetime = datetime.strptime(pub_date[:24], \"%a, %d %b %Y %H:%M:%S\")\n",
    "            date_format = \"%a, %d %b %Y %H:%M:%S %z\"\n",
    "            pub_date_datetime = datetime.strptime(pub_date, date_format)\n",
    "            # print(pub_date_datetime)\n",
    "\n",
    "            # Format the datetime object to get the desired date string\n",
    "            formatted_date_str = pub_date_datetime.strftime('%Y-%m-%d')\n",
    "            formatted_date_str = datetime.strptime(formatted_date_str, \"%Y-%m-%d\")\n",
    "            if formatted_date_str == current_date:\n",
    "                # print(\"--------------------\")\n",
    "                # print(formatted_date_str, current_date)\n",
    "                # Define keywords to search for\n",
    "                keywords = [\"phd\", \"ph.d.\", \" doctoral\", \"ms \", \"m.s.\", \"master\", \"assistantship\", \"fellowship\"]\n",
    "\n",
    "                # Initialize a list to store the links containing the keywords\n",
    "                filtered_links = []\n",
    "\n",
    "                for keyword in keywords:\n",
    "                    if re.search(keyword, title, re.I):\n",
    "                        filtered_links.append(guid)\n",
    "\n",
    "        #         print(len(filtered_links))\n",
    "                # Print the filtered links\n",
    "                # ...\n",
    "                for link in filtered_links:\n",
    "    #                 print(link)\n",
    "    #                 print(current_date)\n",
    "    #                 print(title)\n",
    "    #                 print(\"----------------------\")\n",
    "                    \n",
    "                    sign1 = f\"\\n----- ** {current_date.strftime('%Y-%b-%d')} ** -----\"\n",
    "                    sign2 = f\"\\n--------------------\"\n",
    "                    link1 = f\"\\n{link}\"\n",
    "                    message_text = f\" *{title}* \\n\\n{link1}{sign1}{sign2}\"\n",
    "                    # postbot(bot_token, message_text)\n",
    "                    print(message_text)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    # metjobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *Uppsala University, OX2 PhD research* \n",
      "\n",
      "\n",
      "https://maillists.reading.ac.uk/scripts/wa-READING.exe?A2=MET-JOBS;d18b8226.2311B\n",
      "----- ** 2023-Nov-08 ** -----\n",
      "--------------------\n",
      " *SNSF Ambizione BioPSI PhD student open position* \n",
      "\n",
      "\n",
      "https://maillists.reading.ac.uk/scripts/wa-READING.exe?A2=MET-JOBS;3b101c9f.2311B\n",
      "----- ** 2023-Nov-08 ** -----\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "metjobs(current_date='2023-11-08')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from es_jobs_net import esjobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-November\n",
      " *Noaa Oar Deputy Assistant Administrator For Programs And Administration, Silver Spring Maryland* \n",
      "\n",
      "The announcement for a *Deputy Assistant Administrator for Programs and\n",
      "Administration *position in the  *Office of Oceanic and Atmospheric\n",
      "Research (OAR) *has been posted. The announcement will be open for *36*\n",
      "* days,* from *11/13/23 *to* 12/18/23*. If you are interested in applying\n",
      "for the position, go to USA JOBS website or click on the announcement link\n",
      "below.\n",
      "https://www.usajobs.gov/job/760077900#shorten-link\n",
      "\n",
      "Megan L. Melamed, PhD (*she, her*)\n",
      "Deputy Director\n",
      "NOAA Chemical Sciences Laboratory\n",
      "Boulder, CO, USA\n",
      "303.727.0952 (Mobile)\n",
      "megan.melamed at noaa.gov\n",
      "csl.noaa.gov/staff/megan.melamed/\n",
      "<http://www.esrl.noaa.gov/csl/staff/megan.melamed/>\n",
      "\n",
      "----- ** 2023-11-14 ** -----\n",
      "\n",
      " *Postdoctoral Position: Training Machine Learned Parameterizations Of Subgrid Scale Processes Using Ocean Data Assimilation, Princeton, Nj* \n",
      "\n",
      "The Atmospheric and Oceanic Sciences Program at Princeton University, in association with NOAA's Geophysical Fluid Dynamics Laboratory (GFDL), seeks a postdoctoral or more senior research scientist to conduct research on developing and using machine learned parameterizations developed from ocean-data assimilation increments. The goal is to develop parameterizations of unresolved processes that will improve ocean circulation models. A neural network has already been demonstrated to model the misfit between the ocean model and observations. The new research will involve training a new machine-learned model to represent some missing physics contained within this dataset, interpret the new model, implement it as a parameterization in a global circulation model (MOM6), and evaluate the new parameterization within a global-scale climate model.\n",
      "\n",
      "The work is part of a larger project, M2LInES, covering eleven institutions. The overall goal is to reduce climate model biases at the air-sea/ice interface by improving subgrid physics in the ocean, sea ice and atmosphere components of existing coarse (¼° to 1°) resolution IPCC-class climate models, and their coupling, using machine learning. This part of the research at Princeton University/GFDL will involve working with the SPEAR ocean data assimilation system and the MOM6 ocean circulation model. The prognostic parameterizations will be state-dependent and trained to minimize model-observation misfits with the aim of reducing inherent biases in free-running climate simulations. The research will require analysis and interpretation of model output, the management of large datasets and the application of neural nets or other machine learning techniques to those data. The postdoc will be expected to collaborate with other postdocs at Princeton and with other members of the M2LInES project across multiple institutions.\n",
      "\n",
      "In addition to a quantitative background, the selected candidates will ideally have one or more of the following attributes: a) a background in physical oceanography, or machine learning, or a closely related field; b) experience with ocean-circulation or climate models, or ocean data-assimilation systems; and c) experience, or demonstrated interest, in machine learning.\n",
      "\n",
      "Candidates must have a Ph.D. and preferably in oceanography, geophysical fluid dynamics, computer science, or a closely related field. The initial appointment is for one year with the possibility of a second-year renewal subject to satisfactory performance and available funding.\n",
      "\n",
      "Complete applications, including a cover letter, CV, publication list, research statement (no more than 2 pages incl. references), and 3 letters of recommendation should be submitted by December 22, 2023, 11:59 pm EST for full consideration.\n",
      "\n",
      "Princeton is interested in candidates who, through their research, will contribute to the diversity and excellence of the academic community. Applicants should apply online to https://www.princeton.edu/acad-positions/position/32681. For additional information contact Dr. Feiyu Lu (feiyu.lu at princeton.edu) or Dr. Alistair Adcroft (aadcroft at princeton.edu).\n",
      "\n",
      "The work location for this position is in-person on campus at Princeton University. This position is subject to Princeton University's background check policy which will include meeting the security requirements for accessing the NOAA Geophysical Fluid Dynamics Laboratory. Princeton University is an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity or expression, national origin, disability status, protected veteran status, or any other characteristic protected by law.\n",
      "\n",
      "Anna Valerio :8)\n",
      "Department & Graduate Administrator\n",
      "Princeton University\n",
      "AOS Program\n",
      "300 Forrestal Road, 209 Sayre Hall\n",
      "Princeton, NJ 08540\n",
      "phone:  609-258-6677\n",
      "fax: 609-258-2850\n",
      "e-mail:  apval at princeton.edu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----- ** 2023-11-14 ** -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "esjobs(current_date_hawaii='2023-11-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-November\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'strftime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#X12sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m                         \u001b[39mprint\u001b[39m(message_text)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#X12sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#X12sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m     esjobs(current_date\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m2023-11-12\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#X12sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m date_time_ \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mstrptime(date_time_str, format_str)\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#X12sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39m# print(date_time_, current_date.strftime(format='%Y-%m-%d'))\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#X12sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39mif\u001b[39;00m date_time_\u001b[39m.\u001b[39;49mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m current_date\u001b[39m.\u001b[39mstrftime(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#X12sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m     \u001b[39mprint\u001b[39m(date_time_, current_date\u001b[39m.\u001b[39mstrftime(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/syed44/Git_Stuff/telebot/atmocean/test_nb.ipynb#X12sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m     \u001b[39m# Extract the header\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'strftime'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pytz\n",
    "from job_bot import postbot\n",
    "from api_token import api_key\n",
    "\n",
    "# def init_bot():\n",
    "bot_token = api_key()\n",
    "\n",
    "def esjobs(post_jobs=True, jobbot_status=False, current_date=None):\n",
    "    if jobbot_status:\n",
    "        sign1 = f\"\\n----- ** {datetime.now().strftime('%Y-%b-%d')} ** -----\"\n",
    "        sign2 = f\"\\nEarth Science Jobs\"\n",
    "        message_text = f\" *{'Job_Bot Status: Active'}* {sign1}{sign2}\"\n",
    "        postbot(bot_token, message_text)\n",
    "    \n",
    "    if post_jobs:\n",
    "        if current_date is None:\n",
    "            # Get the current time in the Hawaii timezone\n",
    "            hawaii_timezone = pytz.timezone('Pacific/Honolulu')\n",
    "            current_date_hawaii = datetime.now(hawaii_timezone)    \n",
    "            current_date = current_date_hawaii\n",
    "            current_time = current_date.strftime(\"%B %Y\")\n",
    "        else:\n",
    "            current_date = datetime.strptime(current_date, \"%Y-%m-%d\")\n",
    "            current_time = current_date.strftime(\"%B %Y\")\n",
    "        # print(current_time)\n",
    "        url = \"https://mailman.ucar.edu/pipermail/es_jobs_net/\"\n",
    "\n",
    "        # Define user-agent and headers\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; \\\n",
    "            x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,\\\n",
    "            application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9,ar;q=0.8\",\n",
    "            \"Cache-Control\": \"max-age=0\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Host\": \"mailman.ucar.edu\",\n",
    "            \"If-Modified-Since\": \"Thu, 19 Oct 2023 09:12:44 GMT\",\n",
    "            \"If-None-Match\": \"\\\"1a8c-6080e289aa9c3\\\"\",\n",
    "            \"Sec-Ch-Ua\": \"\\\"Chromium\\\";v=\\\"118\\\", \\\"Google Chrome\\\";v=\\\"118\\\", \\\"Not=A?Brand\\\";v=\\\"99\\\"\",\n",
    "            \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "            \"Sec-Ch-Ua-Platform\": \"\\\"macOS\\\"\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\"\n",
    "        }\n",
    "\n",
    "        # Send a request to the URL\n",
    "        response = requests.get(url, headers=headers)\n",
    "        page_content = response.text\n",
    "\n",
    "        soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "\n",
    "        # Find the link to the latest thread (October 2023)\n",
    "        latest_thread_link = None\n",
    "        for row in soup.find_all(\"tr\"):\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells and current_time in cells[0].get_text():\n",
    "                latest_thread_link = row.find(\"a\", href=True)[\"href\"]\n",
    "                break\n",
    "\n",
    "        tempPath = latest_thread_link.split(\"/\")[0]\n",
    "        print(tempPath)\n",
    "\n",
    "        # Construct the URL for the latest thread\n",
    "        if latest_thread_link:\n",
    "            latest_thread_url = f\"{url.rstrip('/')}/{latest_thread_link}\"\n",
    "\n",
    "\n",
    "            # Send a request to the latest thread's URL and scrape its content\n",
    "            response_latest_thread = requests.get(latest_thread_url, headers=headers)\n",
    "            latest_thread_content = response_latest_thread.text\n",
    "\n",
    "            # Parse and extract data from the latest thread's content using BeautifulSoup\n",
    "            latest_thread_soup = BeautifulSoup(latest_thread_content, \"html.parser\")\n",
    "\n",
    "            # Define keywords to search for\n",
    "            keywords = [\"phd\", \"ph.d.\", \"doctoral\", \"ms \", \"m.s.\", \"master\", \"assistantship\", \"fellowship\"]\n",
    "\n",
    "            # Initialize a list to store the links containing the keywords\n",
    "            filtered_links = []\n",
    "\n",
    "            # Loop through the links and check if they contain any of the keywords\n",
    "            for link in latest_thread_soup.find_all(\"a\", href=True):  # Fixed this line\n",
    "                for keyword in keywords:\n",
    "                    if re.search(keyword, link.get_text(), re.I):\n",
    "                        filtered_links.append(link['href'])\n",
    "\n",
    "            # Print the filtered links\n",
    "            for link in filtered_links:\n",
    "        #             print(f\"posting {link}\")\n",
    "                # print(os.path.join(url, tempPath, link))\n",
    "                full_url = os.path.join(url, tempPath, link)\n",
    "                # Send a request to the filtered link and scrape its content\n",
    "                response_filtered_link = requests.get(full_url, headers=headers)\n",
    "                filtered_link_content = response_filtered_link.text\n",
    "\n",
    "                # Parse and extract data from the filtered link's content using BeautifulSoup\n",
    "                filtered_link_soup = BeautifulSoup(filtered_link_content, \"html.parser\")\n",
    "                # You can perform additional parsing or extraction here, or simply print the content\n",
    "                # print(\"Content of filtered link:\", full_url)        \n",
    "        #         print(filtered_link_soup.get_text())  # Print the text content of the filtered link\n",
    "    #             print(\"--------------------------------------\\n\")\n",
    "                pub_date = filtered_link_soup.find(\"i\").get_text()\n",
    "                format_str = '%a %b %d %H:%M:%S %Y'\n",
    "                date_time_str = pub_date[:19] + pub_date[23:]\n",
    "\n",
    "                if len(date_time_str) == 24:\n",
    "                    date_time_ = datetime.strptime(date_time_str, format_str)\n",
    "                    date_time_str_formatted = date_time_.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                    if date_time_.date() == current_date.date():\n",
    "                        print(date_time_str_formatted, current_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "                        # Extract the header\n",
    "                        header = filtered_link_soup.find(\"h1\")  # Assuming the header is within an h2 element\n",
    "                        if header:\n",
    "                            header_text = header.get_text().removeprefix('[ES_JOBS_NET] ')\n",
    "                            # print(\"Header:\")\n",
    "                            # print(header_text)\n",
    "\n",
    "                        # Extract the body\n",
    "                        body = filtered_link_soup.find(\"pre\")  # Adjust class as needed\n",
    "                        if body:\n",
    "                            body_text = body.get_text()\n",
    "                            # print(\"\\nBody:\")\n",
    "                            split_index = body_text.find(\"-------------- next part --------------\")\n",
    "                            if split_index != -1:\n",
    "                            # Keep only the part before \"-------------- next part --------------\"\n",
    "                                body_text = body_text[:split_index]\n",
    "                            # print(body_text)\n",
    "                        sign1 = f\"\\n----- ** {current_date} ** -----\"\n",
    "                        sign2 = f\"\\n\"\n",
    "                        message_text = f\" *{header_text.title()}* \\n\\n{body_text}{sign1}{sign2}\"\n",
    "                        # postbot(bot_token, message_text)\n",
    "                        print(message_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    esjobs(current_date='2023-11-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radar-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
